{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 1 ноября 2018, 06:00 \n",
    "**Штраф за опоздание:** -2 балла после 06:00 1 ноября, -4 балла после 06:00 8 ноября, -6 баллов после 06:00 15 ноября\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (4 балла)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы fit такой, чтобы она была медленнее sklearn не более чем в 10 раз. Скорость проверяем на  wine и Speed Dating Data. Для ускорения используем только numpy.\n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 3 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\devkits\\anaconda_3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from abc import ABCMeta\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "# %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(probability):\n",
    "    if probability.ndim == 2:\n",
    "        return 1 - (probability ** 2).sum(axis=1).reshape(-1, 1)\n",
    "    else:\n",
    "        return 1 - (probability ** 2).sum()\n",
    "\n",
    "\n",
    "def calc_entropy(probability):\n",
    "    if probability.ndim == 2:\n",
    "        return - np.nan_to_num(probability * np.log(probability)).sum(axis=1).reshape(-1, 1)\n",
    "    else:\n",
    "        return - np.nan_to_num(probability * np.log(probability)).sum()\n",
    "\n",
    "\n",
    "def calc_miscass(probability):\n",
    "    if probability.ndim == 2:\n",
    "        max_probability = probability.max(axis=1)\n",
    "        return 1 - max_probability.reshape(-1, 1)\n",
    "    else:\n",
    "        max_probability = probability.max()\n",
    "        return 1 - max_probability\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier(six.with_metaclass(ABCMeta, BaseEstimator)):\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None,\n",
    "                 sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.class_amount = -1\n",
    "        self.feature_importances_ = None\n",
    "        self.criterion = criterion\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "            self.G_sub_function = calc_gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "            self.G_sub_function = calc_entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "            self.G_sub_function = calc_miscass\n",
    "        else:\n",
    "            raise ValueError('invalid criterion name')\n",
    "\n",
    "        self.max_features = max_features\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            raise ValueError('invalid max_features name')\n",
    "\n",
    "    # I = 1 - ΣPk^2\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        left_part = (l_s / (l_s + r_s))\n",
    "        return (left_part * calc_gini(l_c / l_s) +\n",
    "                (1 - left_part) * calc_gini(r_c / r_s))\n",
    "\n",
    "    # - ΣPk*log(Pk)\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        left_part = (l_s / (l_s + r_s))\n",
    "        return (left_part * calc_entropy(l_c / l_s) +\n",
    "                (1 - left_part) * calc_entropy(r_c / r_s))\n",
    "\n",
    "    # 1 - max(Pk)\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        left_part = (l_s / (l_s + r_s))\n",
    "        return (left_part * calc_miscass(l_c / l_s) +\n",
    "                (1 - left_part) * calc_miscass(r_c / r_s))\n",
    "\n",
    "    # sqrt(n) случайных\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.sqrt(n_feature)]\n",
    "\n",
    "    # log2(n) случайных\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:np.log2(n_feature)]\n",
    "\n",
    "    # все n\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "\n",
    "    # сортируем данные по, убыванию\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    # нужно найти такую величину, которая готова разбить наше множество наилучшим образом\n",
    "    def __find_threshold(self, x, y):\n",
    "        # сортируем по величине значения фичи\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        # количество классов\n",
    "        class_amount = self.class_amount\n",
    "        # число элементов, которые нужно отделить\n",
    "        cut_size = np.int(self.min_samples_split / 2 - 1)\n",
    "\n",
    "        # получаем внутренние элементы\n",
    "        splitted_sorted_y = sorted_y[cut_size:-cut_size + sorted_y.size]\n",
    "        # тут мы делим наши значения на группы\n",
    "        # в r_ids попадают те номера, которые отличаются от последующего (словно \"неправильные\")\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] !=\n",
    "                                splitted_sorted_y[1:])[0] + (cut_size + 1)\n",
    "        # если дальше делить нет смысла\n",
    "        # 1) элементов < чем минимум для деления\n",
    "        # 2) или больше в группе нет \"неправильных\"\n",
    "        if len(r_border_ids) == 0:\n",
    "            return np.inf, None\n",
    "\n",
    "        # считаем колличество подряд идущих элементов исходя из того, что r_border_ids - это позиции !=`х элементов\n",
    "        # получаем, что разница 2х подряд идущих индексов есть количество подряд идущих равных элементов\n",
    "        eq_el_count = r_border_ids - np.append(np.array([cut_size]),\n",
    "                                               r_border_ids[:-1])\n",
    "        # Инициализируем индикаторную матрицу \"неправильных\" элементов\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_amount))\n",
    "        # На позициях тех классов, чьи элементы повторялись устанавливаем 1\n",
    "        # для каждой строки (все строки одинаковы)\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]),\n",
    "                     sorted_y[r_border_ids - 1]] = 1\n",
    "        # умножаем предыдущую матрицу на столбец подряд идущих элементов\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        # для 0 позиции добавляем число \"вырезанных\" элементов\n",
    "        class_increments[0] = class_increments[0] + np.bincount(sorted_y[:cut_size], minlength=class_amount)\n",
    "\n",
    "        # в левую вершину записываем \"Хорошие\" подряд идущие элементы (с помощью коммулятивной суммы)\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        # в правую остальные\n",
    "        r_class_count = np.bincount(sorted_y, minlength=class_amount) - l_class_count\n",
    "        # число элементов слева это номер на котором происходит последняя смена класса\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        # число остальных есть разница всего числа и числа для левой вершины\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # считаем для каждого варианта его меру неопределенности\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        # выбираем лучшее\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # Забираем искомую величину - на каком элементе разобьем\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        # возвращаем неопределенность + значение, на котором нужно разбить выборку\n",
    "        return gs[idx], (sorted_x[left_el_id - 1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        # clear prev\n",
    "        if node_id == 0:\n",
    "            self.tree.clear()\n",
    "        # stop if empty\n",
    "        if y.size == 0:\n",
    "            return\n",
    "\n",
    "        # stop if:\n",
    "        #   ready\n",
    "        #   less then min\n",
    "        #   depth\n",
    "        #   % of max class in cur node is more then sufficient_share\n",
    "        if y.size < self.min_samples_split or \\\n",
    "                depth == self.max_depth or \\\n",
    "                np.unique(y).shape[0] == 1 or \\\n",
    "                np.max(np.bincount(y)) / y.size >= self.sufficient_share:\n",
    "            self.__init_leaf(x, y, node_id)\n",
    "            return\n",
    "\n",
    "        chosen_features = self.get_feature_ids(x.shape[1])\n",
    "\n",
    "        # хранит показатель меры неопределенности + место разделения\n",
    "        thresholds = np.zeros((2, x.shape[1]), dtype=np.float32)\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            if i in chosen_features:\n",
    "                thresholds[:, i] = np.array(self.__find_threshold(x[:, i], y))\n",
    "\n",
    "        # searching for best feature\n",
    "        min_uncertainty = np.min(thresholds[0])\n",
    "        best_features = np.where(thresholds[0] == min_uncertainty)[0]\n",
    "        result = np.random.choice(best_features)\n",
    "        threshold = thresholds[1, result]\n",
    "\n",
    "        # divide samples\n",
    "        x_l, x_r, y_l, y_r = self.__div_samples(x, y, result, threshold)\n",
    "\n",
    "        if y_l.size == 0 or y_r.size == 0:\n",
    "            self.__init_leaf(x, y, node_id)\n",
    "            return\n",
    "\n",
    "        self.__set_importance(y_l, y_r, result)\n",
    "        self.__init_none_leaf(node_id, result, threshold)\n",
    "        self.__fit_node(x_l, y_l, 2 * node_id + 1, depth + 1, result)\n",
    "        self.__fit_node(x_r, y_r, 2 * node_id + 2, depth + 1, result)\n",
    "        return\n",
    "\n",
    "    def __init_leaf(self, x, y, node_id):\n",
    "        node = [self.LEAF_TYPE]\n",
    "        classes_count = np.bincount(y)\n",
    "        node.append(np.argmax(classes_count))\n",
    "        node.append(classes_count / classes_count.sum())\n",
    "        self.tree.update({node_id: node})\n",
    "\n",
    "    def __init_none_leaf(self, node_id, feature_id, threshold):\n",
    "        node = [self.NON_LEAF_TYPE, feature_id, threshold]\n",
    "        self.tree.update({node_id: node})\n",
    "\n",
    "    def __set_importance(self, y_l, y_r, feature_id):\n",
    "        classes_amount = np.append(y_l, y_r).max() + 1\n",
    "        l_c = np.bincount(y_l, minlength=classes_amount)\n",
    "        r_c = np.bincount(y_r, minlength=classes_amount)\n",
    "        self.feature_importances_[feature_id] += \\\n",
    "            (self.G_sub_function((l_c + r_c) / (y_l.size + y_r.size)) -\n",
    "             self.G_function(l_c, y_l.size, r_c, y_r.size))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.feature_importances_ = np.zeros((x.shape[1],), dtype=np.float32)\n",
    "        self.class_amount = np.max(y) + 1\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "\n",
    "    #     def get_params(self, deep=True):\n",
    "    #         out = dict()\n",
    "    #         out['max_depth'] = self.max_depth\n",
    "    #         out['sufficient_share'] = self.sufficient_share\n",
    "    #         out['min_samples_split'] = self.min_samples_split\n",
    "    #         out['criterion'] = self.criterion\n",
    "    #         out['max_features'] = self.max_features\n",
    "    #         print(out)\n",
    "    #         return out\n",
    "\n",
    "    #     def set_params(self, **params):\n",
    "    #         print(**params)\n",
    "    #         for key, value in params.items():\n",
    "    #             self.__setattr__(key, value)\n",
    "    #         return self\n",
    "\n",
    "    def score(self, x, y):\n",
    "        pred_y = self.predict(x)\n",
    "        return 1.0 - (np.bincount(np.abs(y - pred_y))[1:]).sum() / y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8407407407407407"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.890993265993266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Speed Dating Data.csv', encoding='cp1251')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id'], axis=1)\n",
    "df = df.drop(['idg'], axis=1)\n",
    "df = df.drop(['condtn'], axis=1)\n",
    "df = df.drop(['round'], axis=1)\n",
    "df = df.drop(['position', 'positin1'], axis=1)\n",
    "df = df.drop(['order'], axis=1)\n",
    "df = df.drop(['partner'], axis=1)\n",
    "df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "              'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "              'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "             axis=1)\n",
    "df = df.drop(['field'], axis=1)\n",
    "df = df.drop(['undergra'], axis=1)\n",
    "df = df.drop(['from', 'zipcode'], axis=1)\n",
    "df = df.drop(['career'], axis=1)\n",
    "df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "              'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga'], axis=1)\n",
    "df = df.drop(['expnum'], axis=1)\n",
    "df = df.drop(['wave'], axis=1)\n",
    "\n",
    "df = df.dropna(subset=['age'])\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "\n",
    "# df.int_corr = df.int_corr.fillna(method='ffill')\n",
    "# df.field_cd = df.field_cd.fillna(method='ffill')\n",
    "# df.amb1_1 = df.amb1_1.fillna(method='ffill')\n",
    "# df.amb2_1 = df.amb2_1.fillna(method='ffill')\n",
    "# df.amb3_1 = df.amb3_1.fillna(method='ffill')\n",
    "# df.shar1_1 = df.shar1_1.fillna(method='ffill')\n",
    "# df.shar2_1 = df.shar2_1.fillna(method='ffill')\n",
    "# df.fun1_1 = df.fun1_1.fillna(method='ffill')\n",
    "# df.fun3_1 = df.fun3_1.fillna(method='ffill')\n",
    "# df.attr3_1 = df.attr3_1.fillna(method='ffill')\n",
    "# df.sinc3_1 = df.sinc3_1.fillna(method='ffill')\n",
    "# df.intel3_1 = df.intel3_1.fillna(method='ffill')\n",
    "\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(df.mn_sat[df.mn_sat.notna()].mean())\n",
    "df.loc[:, 'income'] = df.income.fillna(df.income[df.income.notna()].mean())\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(df.tuition[df.tuition.notna()].mean())\n",
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "\n",
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "    df = df.drop(feat, axis=1)\n",
    "\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "    (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T / df.loc[:,\n",
    "                                                                                      'temp_totalsum'].T).T * 100\n",
    "\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "    (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T / df.loc[:,\n",
    "                                                                                      'temp_totalsum'].T).T * 100\n",
    "\n",
    "df = df.drop(['temp_totalsum'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid'])\\\n",
    "                                 .drop(['gender'], axis=1)\\\n",
    "                                 .dropna()\n",
    "df_female = df.query('gender == 0').drop_duplicates(subset=['iid'])\\\n",
    "                                   .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1)\\\n",
    "                                   .dropna()\n",
    "        \n",
    "df_female.columns = df_female.columns + '_f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female = df_female.drop(['pid_f'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = df_male.join(df_female.set_index('iid_f'), \n",
    "                       on='pid', \n",
    "                       how='inner')\n",
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc2_1_f</th>\n",
       "      <th>intel2_1_f</th>\n",
       "      <th>fun2_1_f</th>\n",
       "      <th>amb2_1_f</th>\n",
       "      <th>shar2_1_f</th>\n",
       "      <th>attr3_1_f</th>\n",
       "      <th>sinc3_1_f</th>\n",
       "      <th>fun3_1_f</th>\n",
       "      <th>intel3_1_f</th>\n",
       "      <th>amb3_1_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1299.211122</td>\n",
       "      <td>21111.97234</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.211122</td>\n",
       "      <td>21111.97234</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.211122</td>\n",
       "      <td>21111.97234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.211122</td>\n",
       "      <td>21111.97234</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1299.211122</td>\n",
       "      <td>21111.97234</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     match  int_corr  samerace   age  field_cd       mn_sat      tuition  \\\n",
       "100      0      0.14         0  27.0       8.0  1299.211122  21111.97234   \n",
       "110      0      0.54         0  22.0       1.0  1299.211122  21111.97234   \n",
       "120      1      0.16         1  22.0       1.0  1299.211122  21111.97234   \n",
       "130      1      0.61         0  23.0       1.0  1299.211122  21111.97234   \n",
       "140      1      0.21         0  24.0       1.0  1299.211122  21111.97234   \n",
       "\n",
       "     race  imprace  imprelig    ...     sinc2_1_f  intel2_1_f  fun2_1_f  \\\n",
       "100   2.0      7.0       3.0    ...          20.0        15.0      20.0   \n",
       "110   2.0      1.0       1.0    ...          20.0        15.0      20.0   \n",
       "120   4.0      3.0       5.0    ...          20.0        15.0      20.0   \n",
       "130   2.0      1.0       1.0    ...          20.0        15.0      20.0   \n",
       "140   3.0      3.0       1.0    ...          20.0        15.0      20.0   \n",
       "\n",
       "     amb2_1_f  shar2_1_f  attr3_1_f  sinc3_1_f  fun3_1_f  intel3_1_f  amb3_1_f  \n",
       "100       5.0        5.0        6.0        8.0       8.0         8.0       7.0  \n",
       "110       5.0        5.0        6.0        8.0       8.0         8.0       7.0  \n",
       "120       5.0        5.0        6.0        8.0       8.0         8.0       7.0  \n",
       "130       5.0        5.0        6.0        8.0       8.0         8.0       7.0  \n",
       "140       5.0        5.0        6.0        8.0       8.0         8.0       7.0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599, 62)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 62)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 124 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5638909727431858"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 25, 30, 1, 13, 38, 6, 60, 31, 26]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf.feature_importances_.argsort()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 34, 26, 25, 24, 23, 22, 36, 42, 19]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_clf.feature_importances_.argsort()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import GridSearchCV\n",
    "    from sklearn.cross_validation import RandomizedSearchCV\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "RND_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_clf = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(2,10),\n",
    "    'min_samples_split': randint(5, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_my_clf = {\n",
    "    'criterion': ['gini', 'entropy', 'misclass'],\n",
    "    'max_depth': randint(2,10),\n",
    "    'min_samples_split': randint(5, 10),\n",
    "    'sufficient_share': uniform(0, 1),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not me\n",
      "{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_split': 6}\n",
      "0.6231987492171501\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid_clf, n_iter=200, n_jobs=-1,\n",
    "                                   cv=cv, scoring='roc_auc')\n",
    "random_search.fit(X, y)\n",
    "print(\"not me\")\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "model = MyDecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid_my_clf, n_iter=10, n_jobs=-1,\n",
    "                                   cv=cv, scoring='roc_auc')\n",
    "random_search.fit(X, y)\n",
    "print(\"me\")\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
